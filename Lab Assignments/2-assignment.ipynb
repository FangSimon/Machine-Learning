{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Lab Assignment 2\n",
    "\n",
    "In the third assignment you will classify hand-written digits using logistic regression.\n",
    "\n",
    "The assignment follows Andrew Ng's explanation of Logistic Regression and (re)watching his videos could be useful (Week 3)\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning () repository on Github.\n",
    "\n",
    "### Deadline October 26th, 23:59\n",
    "\n",
    "Do not hand in any other files, the Notebook should contain all your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has the dataset built in. The dataset contains images of hand-written digits that are only 8 by 8 pixels, which means the algorithm (logistic regression) should run on every computer.\n",
    "\n",
    "The code in the following cell shows how to work with the digits dataset and  how to visualize it. As you can see the numbers are not very clear in 8x8 pixels images, this means we cannot expect our logistic regression will have a very high classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers shown are: \n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAACjCAYAAABBlE9SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAB9FJREFUeJzt3d1RFN0WBuCeU989ZiBGAEYAGQgRiBFgBkIEaAYQARIB\nGAESgRoBTgRzbk95rF5Lu2nm/ep5LmdP9c/u7rf6Yq3dq81mMwCQ4T/PfQAA9AltgCBCGyCI0AYI\nIrQBgghtgCBCGyDIP0+9g9VqNbkQ/OjoaHT87Oys3Mbd3d2kbfz8+bPcxzaoznMYhuHFixej4535\n/Pz5c/OIntfh4eHoeHUeX79+nbyPJbx//778T3Vdv3//Xm6jOteU56R6Bi4vL8ttVLk0h81ms/r1\nN2/aAEGENkAQoQ0QRGgDBBHaAEGENkCQJy/5m0NVqrS7u1tuoyrxqcqdTk5Oyn1sQxlcp+Tq4OBg\ndLxTwrYN57q/v1/+5/b2dnR8vV6PjnfurSVUz0Cn/KwqC/z48WO5jWrOOyWn26B6njulns/FmzZA\nEKENEERoAwQR2gBBhDZAEKENEERoAwQR2gBBnr25ptMgUTU4dLZRNc9UzSKdfSzRcFIdxxxrO29z\nY8H/6jSUPDw8jI5X1+zDhw9/dExPpVrfudMYUzW+dNbTTmmeqZrpquaaznzO0XjVmfNfedMGCCK0\nAYIIbYAgQhsgiNAGCCK0AYIIbYAgz16nXdVTDkNdN/w3tY5/uo+lVAvVV4vh7+zsTD6GlFrcTi1t\ndW9U27i5ufmTQ3oy1Xl0aoar/3Sue/W8dj7CsYSqDruai6oufhjqe6czF9Xz/DvetAGCCG2AIEIb\nIIjQBggitAGCCG2AIEIbIIjQBggS0VyzRLPHtjQNVAX7VdH/4+Pj5GPoXJMlVMdRNSINQ+9DCWOq\nJo1t0WkwqxpKOh/xqP5Tzfccz1Hnml5cXIyOX11dTT6O09PT0fF3795N3sfveNMGCCK0AYIIbYAg\nQhsgiNAGCCK0AYIIbYAgz16n3anb3N/fn7yfqua32kenhvXfojPfS3w0ologvqqT7Tg+Ph4d35ZF\n/edQnUun/rnqI5j6EY+OzjVZr9ej42/fvh0dnyNzniozvGkDBBHaAEGENkAQoQ0QRGgDBBHaAEGE\nNkAQoQ0Q5NmbazqLt1eF7p2mgKmL4VdNBcyv+uDD4eFhuY29vb3R8evr69Hxm5ubch/VcS7RmNVp\nWqk+JtL5+EU150uca+ejKFOb6Tr7qD6k8FSNWd60AYIIbYAgQhsgiNAGCCK0AYIIbYAgQhsgSESd\ndlWD2qlRrRbt79T8boOq9rNTV/zmzZvR8c5cVLXJc6iuWWeh+uo/1b1TzdUw1PfwErXLnZrgOXoN\nqnOpPoKwLar52tnZKbexxDPwO960AYIIbYAgQhsgiNAGCCK0AYIIbYAgQhsgiNAGCPLszTUdJycn\no+NVE0ZnG/8WnYXsHx4eRsdT5qrTyFHNx+7u7uTjqOar0/w1dcH8znlUc9FpFkn5GEjVBFTNxWq1\nmvNwZuVNGyCI0AYIIrQBgghtgCBCGyCI0AYIIrQBgqw2m83T7mC1mryDapH5ly9fTt3F8OPHj9Hx\nOep553B0dDQ6fn19XW7j/Px8dLxTV7wN5lhwv6rxn6MWfIkPbNzd3ZX/meMerp7FJc61cx7fvn17\n8uOo+h06H+mobDab/ysY96YNEERoAwQR2gBBhDZAEKENEERoAwQR2gBBItbTrtYa7tRpr9fr0fGq\nzrWzTvXUNZE75qihrtYaTjHH2s7VfHZqgpeoTa501pSvaqw766hX93g1F5168krnWax8+fJldLya\nq2F4vuvuTRsgiNAGCCK0AYIIbYAgQhsgiNAGCCK0AYIIbYAgEc01VaH73t5euY2dnZ3R8ao5YYnG\nmY6qsaBamH0Yeo0Y26BqXpijuWGODylUH6a4vLycvI9KZx/39/ej451Gouo56DSlTDXHPqpr1mlA\nm6PJ52940wYIIrQBgghtgCBCGyCI0AYIIrQBgghtgCARddpVTWWnXnd/f390/OLi4k8O6bfmWJS/\nUtWGdmpYq9rkTo3qNtTjVtd0GKbXclf33jDMs7D/VHPUDB8cHJT/efXq1ej4EvdFp2ei6ld4fHwc\nHf/06VO5j+r+69S9/818edMGCCK0AYIIbYAgQhsgiNAGCCK0AYIIbYAgQhsgSERzTWWJ5oZOofwS\nqmL8ToNE1YjRaTR6/fr16PgcH1qozrXT+LLZbEbHj4+PR8e3oXFmGOpGjtvb23Ib5+fno+Ode7xq\nvKquyRLNN8NQz1c1Psf922m269zDv/KmDRBEaAMEEdoAQYQ2QBChDRBEaAMEEdoAQSLqtKtaxs6i\n6GdnZ5OOofNhgCVcXl6OjndqrKta2U69bnVN5qhzrXTqYNfr9ej4ttRhV6prVp3nMNTz1bnu9/f3\no+MnJyej41Ofw7lU92fn3qrO9W9qsDu8aQMEEdoAQYQ2QBChDRBEaAMEEdoAQYQ2QBChDRAkornm\n8PBwdPz09HTyPq6urkbHt6UJo2qu6TRIVE0BnXPdhmaj6r4YhvpcO41Z26A6zs41e3x8HB3vNOjc\n3NyMjneaUpZQHUf1EYTqQyHDUN9/T9Vg5k0bIIjQBggitAGCCG2AIEIbIIjQBggitAGCrDabzXMf\nAwBN3rQBgghtgCBCGyCI0AYIIrQBgghtgCBCGyCI0AYIIrQBgghtgCBCGyCI0AYIIrQBgghtgCBC\nGyCI0AYIIrQBgghtgCBCGyCI0AYI8l9VnUZu8t51VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f71f910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "#Show both rows at the same time\n",
    "plt.imshow(np.vstack((firstrow,secondrow)), interpolation = 'nearest')\n",
    "\n",
    "print \"The numbers shown are: \\n\", np.vstack((digits.target[:5], digits.target[5:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "You have to implement the next three functions and fill in the body of the loop in order to create a correct implementation of logistic regression. Don't change the definitions of the functions and input parameters.\n",
    "\n",
    "Make sure that you do not overfit by keeping track of the score on the test set and implementing a correct stop condition. Also pick a learning rate alpha that makes sure the algorithm learns in a smooth and stable manner.\n",
    "\n",
    "Plot how your score on the test set improves over time. My best score was about 85% correct!\n",
    "\n",
    "Make sure to comment your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a prediction function h\n",
    "def prediction_function(x,theta):\n",
    "# The prediction function takes x and theta as an input, \n",
    "# which are both vectors and outputs a vector. \n",
    "    return 1/(1+exp(-1*np.dot(x, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use the output of that function to compute the cost function J:\n",
    "def safe_log(x): \n",
    "#We define a new function that prevents NaN as an output of the cost_function\n",
    "#The function returns a 0 when it is negative or equal to zero\n",
    "#Otherwise it returns the log value of x\n",
    "    if x <= 0: \n",
    "        return 0\n",
    "    return np.log(x)\n",
    "\n",
    "def cost_function(x_predict,y):  \n",
    "# The cost_function takes in two vectors x_predict and y, \n",
    "# and outputs a scalar, which gives error of x_predict. \n",
    "# The function calculates the following:\n",
    "# J(theta) = (1/m) * sum_{i=1}^m (y^i * log(h(x^i))) + (1-y^i)log(1-h(x^i))\n",
    "    m = len(y)\n",
    "    J = 0 \n",
    "    for i in range(m):\n",
    "        temp = y[i] * safe_log(x_predict[i]) + (1-y[i]) * safe_log(1-x_predict[i])\n",
    "        J = J + temp\n",
    "    return (-1/m) * J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "def compute_gradient(x_predict, y, x):\n",
    "# For the gradient descent, we have to update the theta values simultaneously\n",
    "# Therefore, we have to store all the gradient values in one matrix and \n",
    "# the gradient value for each theta_j is given by \n",
    "# (x^i - y^i)x_j\n",
    "    gradJ = np.zeros((64,10))\n",
    "    m = len(y)\n",
    "    for i in range(m):\n",
    "        gradJ[:,i] = (x_predict[i] - y[i])*x\n",
    "    return gradJ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of correct predictions: 251 digits\n",
      "total number of incorrect predictions: 46 digits\n",
      "accuracy: 85.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e4d41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Choose a suitable learning rate\n",
    "alpha = 0.001 # alpha = 0.001 gives us an accuracy of 85%. Lowering alpha gives us a higher accuracy\n",
    "iterations = 25\n",
    "theta = np.zeros((64,10))\n",
    "\n",
    "#It is important to check that you're not overfitting by testing your prediction on a testset\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "\n",
    "# cost_vec = np.zeros(1500)\n",
    "\n",
    "for i in range(iterations):\n",
    "    for j in range(x.shape[0]):\n",
    "        x_predict = prediction_function(x[j,:].T,theta)\n",
    "        y = np.zeros(10)\n",
    "        y[target[j]] = 1\n",
    "        \n",
    "#        cost_vec[j] = cost_function(x_predict,y)\n",
    "        # we update the theta simulatenously and store each theta \n",
    "        theta = theta - alpha * compute_gradient(x_predict, y, x[j,:])\n",
    "        \n",
    "totalIterations = shape(x_test)[0]\n",
    "counter = 0\n",
    "accuracy_vec = 0 \n",
    "for i in range(totalIterations):\n",
    "    x_predict = prediction_function(x_test[i], theta)\n",
    "    if argmax(x_predict) == target_test[i]:\n",
    "        counter = counter + 1\n",
    "        accuracy = (counter/(i+1))*100\n",
    "    accuracy_vec = np.append(accuracy_vec, accuracy)\n",
    "        \n",
    "plt.plot(accuracy_vec)\n",
    "\n",
    "wrong = totalIterations - counter\n",
    "percent = round((counter/totalIterations)*100)\n",
    "print \"total number of correct predictions: \" + str(counter) + \" digits\"\n",
    "print \"total number of incorrect predictions: \" + str(wrong) + \" digits\"\n",
    "print \"accuracy: \" + str(percent) + \"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
